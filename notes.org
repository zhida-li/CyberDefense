
* headers
  HoursMins Hours Mins Seconds Ann With APrefix WPrefix AvgPath MaxPath UniqAvgPag DupAnn ImpWith DupWith MaxEditDist AvgEditDist 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 IGP EGP Inc AvgSz
  



* stage 1  - download

  - download file to ./src/data_ripe/DATE/updates.*.gz
    (wget -e robots=off -r -np -nd -A 'data_file' data_link)
  - rename to updates.*.Z
  - run zebra to produce DUMP_<DATE>

* stage 2 - feature extraction

  - for each DUMP_<DATE> file
      - move it into C# code (CSharp_Tool_BGP/ConsoleApplication/bin/Release/) as "DUMP"
      - run the console app, it reads DUMP and produces DUMP_out.txt
      - rename it DUMP_<DATE>_out.txt
      - copy them to src/data_split

* stage 3 - generate labels

  - use src/data_split/DUMP_<DATE>_out.txt
  - create src/STAT/labels_RIPE.csv

* stage 4 - data partition

  - use src/data_split/DUMP_<DATE>_out.txt
  - create src/data_split/{train,test}_<CUT>_RIPE.csv
  - create src/STAT/train_test_stat.txt

* stage 5 - normalize train test

  - use src/data_split/{train,test}_<CUT>_RIPE.csv
  - create src/data_split/{train,test>_<CUT>_RIPE_n.csv

* stage 6 - run

  - use src/data_split/{train,test}_<CUT>_RIPE_n
  - copy to src/RNN_Running_Code/RNN_Run/dataset
  - rename to train.csv, test.csv
  - in src/RNN_Running_Code run ./integrate_run.sh
     - changes to each sub-directory that contains a save_memory_run.sh and runs it
       (probably just RNN_Run)
       - sets up a number of directories of python code to run (gru_Klayer_batchM...) and
	 runs thme    
  - in src/RNN_Running_Code/Run run ./collect.sh
     - copies *_accurancy.txt and *_runtime.txt to directories res_acc and res_run resp.
  - cp res_acc res_run to ../data_representation
  - in src/RNN_Running_Code/data_representation run TableGenerator.py
    

  
